import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

#  Modelado y evaluación
# -----------------------------------------------------------------------------
from sklearn.model_selection import train_test_split

# Funciones de lectura de las divisas a usar
def reading_euros():
    list_euros = [3380, 3268, 3468, 3777, 411, 1807, 3583, 1086, 1516, 925, 4409, 2942, 4981, 4209, 4868, 3144, 3337, 192, 4439, 1880, 804, 1779, 1273, 4707, 133, 2532, 3485, 1714, 4258, 4928, 605, 2967, 1277, 4114, 232, 1901, 2730, 3230, 1178, 2208, 2479, 4375, 1144, 9, 4873, 785, 3756, 1839, 2959, 2753, 1282, 2683, 3042, 49, 1610, 4590, 2125, 3295, 3025, 2001, 1705, 1395, 4574, 462, 4190, 1104, 4753, 2769, 2138, 4329, 4537, 1406, 491, 4661, 1023, 692, 4876, 1811, 3306, 3325, 4031, 340, 2952, 116, 2225, 951, 3659, 3131, 2150, 1879, 2134, 4121, 136, 1794, 3988, 1157, 181, 2782, 15, 4305, 1590, 3462, 3204, 3888, 1773, 780, 2913, 3002, 2506, 2830, 706, 1453, 455, 1321, 4594, 3637, 1759, 4663, 2482, 3227, 3681, 50, 845, 4315, 3439, 2134, 2632, 2981, 4971, 1024, 26, 3784, 2689, 1238, 1568, 4601, 2622, 3122, 619, 2687, 1162, 4892, 4314, 4917, 4786, 507, 258, 2902, 692, 1014, 405, 2462, 18, 3559, 3927, 4071, 1915, 4930, 3845, 4948, 1429, 2972, 2099, 878, 608, 798, 4164, 1158, 3330, 1442, 4041, 2971, 3363, 3119, 3213, 4429, 4794, 2076, 2973, 1153, 315, 68, 2340, 4054, 4708, 3516, 2886, 2165, 3739, 979, 1859, 1894, 1966, 4919, 871, 889, 1221, 2545, 659, 969, 2963, 3894, 3954, 4915, 3950, 1498, 1056, 3067, 1686, 1263, 4681, 3537, 1069, 3085, 365, 1024, 306, 2104, 298, 1725, 454, 3507, 2773, 4827, 2308, 3091, 4204, 2033, 2597, 2301, 4474, 2824, 1222, 2885, 2678, 900, 3032, 3691, 85, 287, 3028, 448, 3022, 2642, 4056, 979, 3484, 2088, 4815, 4021, 2597, 813, 3380, 4906, 3048, 3247, 4336, 2499, 2772, 2169, 323, 649, 4334, 4021, 2987, 2998, 3006, 3506, 4461, 2672, 610, 3075, 392, 938, 3413, 729, 2086, 2131, 2628, 3712, 3604, 4915, 543, 834, 516, 2491, 4572, 4969, 4027, 4511, 3394, 464, 3182, 291, 3155, 25, 4766, 3021, 3905, 380, 1885, 221, 3967, 324, 4233, 3661, 2, 4893, 4642, 157, 4356, 571, 3740, 4197, 4725, 3044, 1984, 4907, 3145, 2822, 1426, 1761, 436, 2657, 2547, 3375, 4946, 1775, 3675, 2595, 1603, 4958, 1524, 2473, 802, 551, 3067, 3588, 1469, 4759, 3648, 1682, 1283, 3397, 458, 1411, 2279, 4533, 4675, 1607, 2592, 4729, 2051, 1493, 3934, 3854, 1817, 401, 3531, 804, 1571, 4446, 887, 2244, 4347, 2862, 4679, 3055, 1057, 83, 4356, 834, 2604, 3907, 1507, 3085, 2733, 3530, 4369, 940, 2348, 312, 481, 3409, 1002, 945, 1349, 211, 3783, 307, 3740, 525, 3430, 3163, 1, 4195, 2156, 1754, 2772, 826, 4329, 2131, 4802, 684, 3731, 1203, 4044, 2424, 2349, 4131, 821, 4649, 3646, 4863, 2886, 3479, 1148, 2750, 1886, 2404, 1141, 2357, 3495, 3911, 3930, 1138, 3389, 91, 67, 3828, 4566, 3339, 4370, 4713, 1608, 3468, 1035, 4138, 2917, 142, 1089, 4228, 2539, 941, 2252, 4803, 603, 3802, 2469, 4246, 4816, 2714, 4817, 3191, 497, 2061, 1201, 898, 3542, 143, 4905, 1331, 38, 3650, 4312, 125, 737, 4603, 2963, 4241, 215, 4641, 2101, 2716, 738, 2126, 1698, 402, 585, 3380, 721, 1377, 85, 1981, 13, 2743, 2845, 3347, 2118, 1461, 3732, 1046, 1122, 2956, 4097, 4764, 2770, 2948, 4657, 297]
    euros = np.array(list_euros, dtype=float)
    return euros

def reading_dolars():
    list_dolars = [3887.0, 3758.2, 3988.2, 4343.55, 472.65, 2078.05, 4120.45, 1248.9, 1743.4, 1063.75, 5070.35, 3383.3, 5728.15, 4840.35, 5598.2, 3615.6, 3837.55, 220.8, 5104.85, 2162.0, 924.6, 2045.85, 1463.95, 5413.05, 152.95, 2911.8, 4007.75, 1971.1, 4896.7, 5667.2, 695.75, 3412.05, 1468.55, 4731.1, 266.8, 2186.15, 3139.5, 3714.5, 1354.7, 2539.2, 2850.85, 5031.25, 1315.6, 10.35, 5603.95, 902.75, 4319.4, 2114.85, 3402.85, 3165.95, 1474.3, 3085.45, 3498.3, 56.35, 1851.5, 5278.5, 2443.75, 3789.25, 3478.75, 2301.15, 1960.75, 1604.25, 5260.1, 531.3, 4818.5, 1269.6, 5465.95, 3184.35, 2458.7, 4978.35, 5217.55, 1616.9, 564.65, 5360.15, 1176.45, 795.8, 5607.4, 2082.65, 3801.9, 3823.75, 4635.65, 391.0, 3394.8, 133.4, 2558.75, 1093.65, 4207.85, 3600.65, 2472.5, 2160.85, 2454.1, 4739.15, 156.4, 2063.1, 4586.2, 1330.55, 208.15, 3199.3, 17.25, 4950.75, 1828.5, 3981.3, 3684.6, 4471.2, 2038.95, 897.0, 3349.95, 3452.3, 2881.9, 3254.5, 811.9, 1670.95, 523.25, 1519.15, 5283.1, 4182.55, 2022.85, 5362.45, 2854.3, 3711.05, 4233.15, 57.5, 971.75, 4962.25, 3954.85, 2454.1, 3026.8, 3428.15, 5716.65, 1177.6, 29.9, 4351.6, 3092.35, 1423.7, 1803.2, 5291.15, 3015.3, 3590.3, 711.85, 3090.05, 1336.3, 5625.8, 4961.1, 5654.55, 5503.9, 583.05, 296.7, 3337.3, 795.8, 1166.1, 465.75, 2831.3, 20.7, 4092.85, 4516.05, 4681.65, 2202.25, 5669.5, 4421.75, 5690.2, 1643.35, 3417.8, 2413.85, 1009.7, 699.2, 917.7, 4788.6, 1331.7, 3829.5, 1658.3, 4647.15, 3416.65, 3867.45, 3586.85, 3694.95, 5093.35, 5513.1, 2387.4, 3418.95, 1325.95, 362.25, 78.2, 2691.0, 4662.1, 5414.2, 4043.4, 3318.9, 2489.75, 4299.85, 1125.85, 2137.85, 2178.1, 2260.9, 5656.85, 1001.65, 1022.35, 1404.15, 2926.75, 757.85, 1114.35, 3407.45, 4478.1, 4547.1, 5652.25, 4542.5, 1722.7, 1214.4, 3527.05, 1938.9, 1452.45, 5383.15, 4067.55, 1229.35, 3547.75, 419.75, 1177.6, 351.9, 2419.6, 342.7, 1983.75, 522.1, 4033.05, 3188.95, 5551.05, 2654.2, 3554.65, 4834.6, 2337.95, 2986.55, 2646.15, 5145.1, 3247.6, 1405.3, 3317.75, 3079.7, 1035.0, 3486.8, 4244.65, 97.75, 330.05, 3482.2, 515.2, 3475.3, 3038.3, 4664.4, 1125.85, 4006.6, 2401.2, 5537.25, 4624.15, 2986.55, 934.95, 3887.0, 5641.9, 3505.2, 3734.05, 4986.4, 2873.85, 3187.8, 2494.35, 371.45, 746.35, 4984.1, 4624.15, 3435.05, 3447.7, 3456.9, 4031.9, 5130.15, 3072.8, 701.5, 3536.25, 450.8, 1078.7, 3924.95, 838.35, 2398.9, 2450.65, 3022.2, 4268.8, 4144.6, 5652.25, 624.45, 959.1, 593.4, 2864.65, 5257.8, 5714.35, 4631.05, 5187.65, 3903.1, 533.6, 3659.3, 334.65, 3628.25, 28.75, 5480.9, 3474.15, 4490.75, 437.0, 2167.75, 254.15, 4562.05, 372.6, 4867.95, 4210.15, 2.3, 5626.95, 5338.3, 180.55, 5009.4, 656.65, 4301.0, 4826.55, 5433.75, 3500.6, 2281.6, 5643.05, 3616.75, 3245.3, 1639.9, 2025.15, 501.4, 3055.55, 2929.05, 3881.25, 5687.9, 2041.25, 4226.25, 2984.25, 1843.45, 5701.7, 1752.6, 2843.95, 922.3, 633.65, 3527.05, 4126.2, 1689.35, 5472.85, 4195.2, 1934.3, 1475.45, 3906.55, 526.7, 1622.65, 2620.85, 5212.95, 5376.25, 1848.05, 2980.8, 5438.35, 2358.65, 1716.95, 4524.1, 4432.1, 2089.55, 461.15, 4060.65, 924.6, 1806.65, 5112.9, 1020.05, 2580.6, 4999.05, 3291.3, 5380.85, 3513.25, 1215.55, 95.45, 5009.4, 959.1, 2994.6, 4493.05, 1733.05, 3547.75, 3142.95, 4059.5, 5024.35, 1081.0, 2700.2, 358.8, 553.15, 3920.35, 1152.3, 1086.75, 1551.35, 242.65, 4350.45, 353.05, 4301.0, 603.75, 3944.5, 3637.45, 1.15, 4824.25, 2479.4, 2017.1, 3187.8, 949.9, 4978.35, 2450.65, 5522.3, 786.6, 4290.65, 1383.45, 4650.6, 2787.6, 2701.35, 4750.65, 944.15, 5346.35, 4192.9, 5592.45, 3318.9, 4000.85, 1320.2, 3162.5, 2168.9, 2764.6, 1312.15, 2710.55, 4019.25, 4497.65, 4519.5, 1308.7, 3897.35, 104.65, 77.05, 4402.2, 5250.9, 3839.85, 5025.5, 5419.95, 1849.2, 3988.2, 1190.25, 4758.7, 3354.55, 163.3, 1252.35, 4862.2, 2919.85, 1082.15, 2589.8, 5523.45, 693.45, 4372.3, 2839.35, 4882.9, 5538.4, 3121.1, 5539.55, 3669.65, 571.55, 2370.15, 1381.15, 1032.7, 4073.3, 164.45, 5640.75, 1530.65, 43.7, 4197.5, 4958.8, 143.75, 847.55, 5293.45, 3407.45, 4877.15, 247.25, 5337.15, 2416.15, 3123.4, 848.7, 2444.9, 1952.7, 462.3, 672.75, 3887.0, 829.15, 1583.55, 97.75, 2278.15, 14.95, 3154.45, 3271.75, 3849.05, 2435.7, 1680.15, 4291.8, 1202.9, 1290.3, 3399.4, 4711.55, 5478.6, 3185.5, 3390.2, 5355.55, 341.55]
    dolars = np.array(list_dolars, dtype=float)
    return dolars

def reading_pounds():
    list_pounds = [2873.0, 2777.8, 2947.8, 3210.45, 349.35, 1535.95, 3045.55, 923.1, 1288.6, 786.25, 3747.65, 2500.7, 4233.85, 3577.65, 4137.8, 2672.4, 2836.45, 163.2, 3773.15, 1598.0, 683.4, 1512.15, 1082.05, 4000.95, 113.05, 2152.2, 2962.25, 1456.9, 3619.3, 4188.8, 514.25, 2521.95, 1085.45, 3496.9, 197.2, 1615.85, 2320.5, 2745.5, 1001.3, 1876.8, 2107.15, 3718.75, 972.4, 7.65, 4142.05, 667.25, 3192.6, 1563.15, 2515.15, 2340.05, 1089.7, 2280.55, 2585.7, 41.65, 1368.5, 3901.5, 1806.25, 2800.75, 2571.25, 1700.85, 1449.25, 1185.75, 3887.9, 392.7, 3561.5, 938.4, 4040.05, 2353.65, 1817.3, 3679.65, 3856.45, 1195.1, 417.35, 3961.85, 869.55, 588.2, 4144.6, 1539.35, 2810.1, 2826.25, 3426.35, 289.0, 2509.2, 98.6, 1891.25, 808.35, 3110.15, 2661.35, 1827.5, 1597.15, 1813.9, 3502.85, 115.6, 1524.9, 3389.8, 983.45, 153.85, 2364.7, 12.75, 3659.25, 1351.5, 2942.7, 2723.4, 3304.8, 1507.05, 663.0, 2476.05, 2551.7, 2130.1, 2405.5, 600.1, 1235.05, 386.75, 1122.85, 3904.9, 3091.45, 1495.15, 3963.55, 2109.7, 2742.95, 3128.85, 42.5, 718.25, 3667.75, 2923.15, 1813.9, 2237.2, 2533.85, 4225.35, 870.4, 22.1, 3216.4, 2285.65, 1052.3, 1332.8, 3910.85, 2228.7, 2653.7, 526.15, 2283.95, 987.7, 4158.2, 3666.9, 4179.45, 4068.1, 430.95, 219.3, 2466.7, 588.2, 861.9, 344.25, 2092.7, 15.3, 3025.15, 3337.95, 3460.35, 1627.75, 4190.5, 3268.25, 4205.8, 1214.65, 2526.2, 1784.15, 746.3, 516.8, 678.3, 3539.4, 984.3, 2830.5, 1225.7, 3434.85, 2525.35, 2858.55, 2651.15, 2731.05, 3764.65, 4074.9, 1764.6, 2527.05, 980.05, 267.75, 57.8, 1989.0, 3445.9, 4001.8, 2988.6, 2453.1, 1840.25, 3178.15, 832.15, 1580.15, 1609.9, 1671.1, 4181.15, 740.35, 755.65, 1037.85, 2163.25, 560.15, 823.65, 2518.55, 3309.9, 3360.9, 4177.75, 3357.5, 1273.3, 897.6, 2606.95, 1433.1, 1073.55, 3978.85, 3006.45, 908.65, 2622.25, 310.25, 870.4, 260.1, 1788.4, 253.3, 1466.25, 385.9, 2980.95, 2357.05, 4102.95, 1961.8, 2627.35, 3573.4, 1728.05, 2207.45, 1955.85, 3802.9, 2400.4, 1038.7, 2452.25, 2276.3, 765.0, 2577.2, 3137.35, 72.25, 243.95, 2573.8, 380.8, 2568.7, 2245.7, 3447.6, 832.15, 2961.4, 1774.8, 4092.75, 3417.85, 2207.45, 691.05, 2873.0, 4170.1, 2590.8, 2759.95, 3685.6, 2124.15, 2356.2, 1843.65, 274.55, 551.65, 3683.9, 3417.85, 2538.95, 2548.3, 2555.1, 2980.1, 3791.85, 2271.2, 518.5, 2613.75, 333.2, 797.3, 2901.05, 619.65, 1773.1, 1811.35, 2233.8, 3155.2, 3063.4, 4177.75, 461.55, 708.9, 438.6, 2117.35, 3886.2, 4223.65, 3422.95, 3834.35, 2884.9, 394.4, 2704.7, 247.35, 2681.75, 21.25, 4051.1, 2567.85, 3319.25, 323.0, 1602.25, 187.85, 3371.95, 275.4, 3598.05, 3111.85, 1.7, 4159.05, 3945.7, 133.45, 3702.6, 485.35, 3179.0, 3567.45, 4016.25, 2587.4, 1686.4, 4170.95, 2673.25, 2398.7, 1212.1, 1496.85, 370.6, 2258.45, 2164.95, 2868.75, 4204.1, 1508.75, 3123.75, 2205.75, 1362.55, 4214.3, 1295.4, 2102.05, 681.7, 468.35, 2606.95, 3049.8, 1248.65, 4045.15, 3100.8, 1429.7, 1090.55, 2887.45, 389.3, 1199.35, 1937.15, 3853.05, 3973.75, 1365.95, 2203.2, 4019.65, 1743.35, 1269.05, 3343.9, 3275.9, 1544.45, 340.85, 3001.35, 683.4, 1335.35, 3779.1, 753.95, 1907.4, 3694.95, 2432.7, 3977.15, 2596.75, 898.45, 70.55, 3702.6, 708.9, 2213.4, 3320.95, 1280.95, 2622.25, 2323.05, 3000.5, 3713.65, 799.0, 1995.8, 265.2, 408.85, 2897.65, 851.7, 803.25, 1146.65, 179.35, 3215.55, 260.95, 3179.0, 446.25, 2915.5, 2688.55, 0.85, 3565.75, 1832.6, 1490.9, 2356.2, 702.1, 3679.65, 1811.35, 4081.7, 581.4, 3171.35, 1022.55, 3437.4, 2060.4, 1996.65, 3511.35, 697.85, 3951.65, 3099.1, 4133.55, 2453.1, 2957.15, 975.8, 2337.5, 1603.1, 2043.4, 969.85, 2003.45, 2970.75, 3324.35, 3340.5, 967.3, 2880.65, 77.35, 56.95, 3253.8, 3881.1, 2838.15, 3714.5, 4006.05, 1366.8, 2947.8, 879.75, 3517.3, 2479.45, 120.7, 925.65, 3593.8, 2158.15, 799.85, 1914.2, 4082.55, 512.55, 3231.7, 2098.65, 3609.1, 4093.6, 2306.9, 4094.45, 2712.35, 422.45, 1751.85, 1020.85, 763.3, 3010.7, 121.55, 4169.25, 1131.35, 32.3, 3102.5, 3665.2, 106.25, 626.45, 3912.55, 2518.55, 3604.85, 182.75, 3944.85, 1785.85, 2308.6, 627.3, 1807.1, 1443.3, 341.7, 497.25, 2873.0, 612.85, 1170.45, 72.25, 1683.85, 11.05, 2331.55, 2418.25, 2844.95, 1800.3, 1241.85, 3172.2, 889.1, 953.7, 2512.6, 3482.45, 4049.4, 2354.5, 2505.8, 3958.45, 252.45]
    pounds = np.array(list_pounds, dtype=float)
    return pounds

def reading_yen():
    list_yen = [439400, 424840, 450840, 491010, 53430, 234910, 465790, 141180, 197080, 120250, 573170, 382460, 647530, 547170, 632840, 408720, 433810, 24960, 577070, 244400, 104520, 231270, 165490, 611910, 17290, 329160, 453050, 222820, 553540, 640640, 78650, 385710, 166010, 534820, 30160, 247130, 354900, 419900, 153140, 287040, 322270, 568750, 148720, 1170, 633490, 102050, 488280, 239070, 384670, 357890, 166660, 348790, 395460, 6370, 209300, 596700, 276250, 428350, 393250, 260130, 221650, 181350, 594620, 60060, 544700, 143520, 617890, 359970, 277940, 562770, 589810, 182780, 63830, 605930, 132990, 89960, 633880, 235430, 429780, 432250, 524030, 44200, 383760, 15080, 289250, 123630, 475670, 407030, 279500, 244270, 277420, 535730, 17680, 233220, 518440, 150410, 23530, 361660, 1950, 559650, 206700, 450060, 416520, 505440, 230490, 101400, 378690, 390260, 325780, 367900, 91780, 188890, 59150, 171730, 597220, 472810, 228670, 606190, 322660, 419510, 478530, 6500, 109850, 560950, 447070, 277420, 342160, 387530, 646230, 133120, 3380, 491920, 349570, 160940, 203840, 598130, 340860, 405860, 80470, 349310, 151060, 635960, 560820, 639210, 622180, 65910, 33540, 377260, 89960, 131820, 52650, 320060, 2340, 462670, 510510, 529230, 248950, 640900, 499850, 643240, 185770, 386360, 272870, 114140, 79040, 103740, 541320, 150540, 432900, 187460, 525330, 386230, 437190, 405470, 417690, 575770, 623220, 269880, 386490, 149890, 40950, 8840, 304200, 527020, 612040, 457080, 375180, 281450, 486070, 127270, 241670, 246220, 255580, 639470, 113230, 115570, 158730, 330850, 85670, 125970, 385190, 506220, 514020, 638950, 513500, 194740, 137280, 398710, 219180, 164190, 608530, 459810, 138970, 401050, 47450, 133120, 39780, 273520, 38740, 224250, 59020, 455910, 360490, 627510, 300040, 401830, 546520, 264290, 337610, 299130, 581620, 367120, 158860, 375050, 348140, 117000, 394160, 479830, 11050, 37310, 393640, 58240, 392860, 343460, 527280, 127270, 452920, 271440, 625950, 522730, 337610, 105690, 439400, 637780, 396240, 422110, 563680, 324870, 360360, 281970, 41990, 84370, 563420, 522730, 388310, 389740, 390780, 455780, 579930, 347360, 79300, 399750, 50960, 121940, 443690, 94770, 271180, 277030, 341640, 482560, 468520, 638950, 70590, 108420, 67080, 323830, 594360, 645970, 523510, 586430, 441220, 60320, 413660, 37830, 410150, 3250, 619580, 392730, 507650, 49400, 245050, 28730, 515710, 42120, 550290, 475930, 260, 636090, 603460, 20410, 566280, 74230, 486200, 545610, 614250, 395720, 257920, 637910, 408850, 366860, 185380, 228930, 56680, 345410, 331110, 438750, 642980, 230750, 477750, 337350, 208390, 644540, 198120, 321490, 104260, 71630, 398710, 466440, 190970, 618670, 474240, 218660, 166790, 441610, 59540, 183430, 296270, 589290, 607750, 208910, 336960, 614770, 266630, 194090, 511420, 501020, 236210, 52130, 459030, 104520, 204230, 577980, 115310, 291720, 565110, 372060, 608270, 397150, 137410, 10790, 566280, 108420, 338520, 507910, 195910, 401050, 355290, 458900, 567970, 122200, 305240, 40560, 62530, 443170, 130260, 122850, 175370, 27430, 491790, 39910, 486200, 68250, 445900, 411190, 130, 545350, 280280, 228020, 360360, 107380, 562770, 277030, 624260, 88920, 485030, 156390, 525720, 315120, 305370, 537030, 106730, 604370, 473980, 632190, 375180, 452270, 149240, 357500, 245180, 312520, 148330, 306410, 454350, 508430, 510900, 147940, 440570, 11830, 8710, 497640, 593580, 434070, 568100, 612690, 209040, 450840, 134550, 537940, 379210, 18460, 141570, 549640, 330070, 122330, 292760, 624390, 78390, 494260, 320970, 551980, 626080, 352820, 626210, 414830, 64610, 267930, 156130, 116740, 460460, 18590, 637650, 173030, 4940, 474500, 560560, 16250, 95810, 598390, 385190, 551330, 27950, 603330, 273130, 353080, 95940, 276380, 220740, 52260, 76050, 439400, 93730, 179010, 11050, 257530, 1690, 356590, 369850, 435110, 275340, 189930, 485160, 135980, 145860, 384280, 532610, 619320, 360100, 383240, 605410, 38610]
    yen = np.array(list_yen, dtype=float)
    return yen

# Conjunto de entrenamiento
def get_data_train(euros,dolars):
    # Obtenemos el 60% del conjunto de datos como conjunto de entrenamiento. Coloca el 40% restante en variables temporales."
    x_train, x_, y_train, y_ = train_test_split(euros, dolars, test_size=0.40, random_state=42)

    print(f"the shape of the training set (input) is: {x_train.shape}")
    print(f"the shape of the training set (target) is: {y_train.shape}\n")

    return x_train, y_train

def euros_dolars():
    euros = reading_euros()
    dolars = reading_dolars()
    get_data_train(euros, dolars)

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(euros, dolars, epochs=1000, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántos euros quieres convertir a dolars? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"{moneda}€ son : ${round(num_float, 2)}")

def dolars_euros():
    euros = reading_euros()
    dolars = reading_dolars()
    get_data_train(dolars, euros)

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(dolars, euros, epochs=1000, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántos dolars quieres convertir a euros? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"${moneda} son : {round(num_float, 2)}€")

def euros_pounds():
    euros = reading_euros()
    pounds = reading_pounds()
    get_data_train(euros, pounds)

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(euros, pounds, epochs=1000, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántos euros quieres convertir a pounds? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"{moneda}€ son : £{round(num_float, 2)}")

def pounds_euros():
    euros = reading_euros()
    pounds = reading_pounds()
    get_data_train(pounds, euros)

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(pounds, euros, epochs=1000, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántas pounds quieres convertir a euros? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"£{moneda} son : {round(num_float, 2)}€")

def euros_yen():
    euros = reading_euros()
    yen = reading_yen()
    get_data_train(euros, yen)
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(euros, yen, epochs=3500, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántos euros quieres convertir a yenes? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"{moneda}€ son : {round(num_float, 2)} yenes")

def yen_euros():
    euros = reading_euros()
    yen = reading_yen()
    get_data_train(yen, euros)
    
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=[1], activation='linear')
        ])
    model.compile(loss = tf.keras.losses.MeanSquaredError(),
                  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01))

    print('Comienza el entrenamiento')
    model.fit(yen, euros, epochs=3500, verbose=False)
    print('Modelo entrenado')
    print(model.get_weights())

    print("Hagamos una predicción")
    moneda = float(input("¿Cuántas yenes quieres convertir a euros? "))
    resultado = model.predict([moneda])
    valor = resultado.item()
    num_float = float(valor)
    print("-------------------")
    print(f"{moneda} yenes son : {round(num_float, 2)}€")